{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from models.transformer.transformer import Transformer\n",
    "from models.custom_metrics.metrics import loss_function, accuracy_function\n",
    "from translator.translator import TranslatorWithBeamSearch\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = path.join(\"dataset\",\"dialogs.txt\")\n",
    "path = path.join(\"dataset\",\"glove.6B.100d.txt\")\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pipeline.create_dataset(data_path,BATCH_SIZE = 128)\n",
    "vectorizer = TextVectorization(max_tokens=5000,standardize=pipeline.add_start_and_end_tokens)\n",
    "vectorizer.adapt(train.map(lambda x: x[\"question\"]))\n",
    "vocab = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "model_dim = 100\n",
    "dff = 256\n",
    "num_heads = 4\n",
    "dropout_rate = 0.2\n",
    "vocab_len = len(vocab)\n",
    "INIT_LR = 1e-4\n",
    "MAX_LR = 1e-2\n",
    "MAX_TOKENS = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    model_dim=model_dim,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_len,\n",
    "    target_vocab_size=vocab_len,\n",
    "    dropout_rate=dropout_rate,\n",
    "    max_tokens=MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer._load_embedding(path,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, model_dim, warmup_steps=1000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.model_dim = tf.cast(self.model_dim, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.model_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(model_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate, \n",
    "                            beta_1=0.9, \n",
    "                            beta_2=0.98,\n",
    "                            epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer([inp, tar_inp],\n",
    "                    training = True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for batch, inputs in enumerate(train):\n",
    "        train_step(vectorizer(inputs[\"question\"]), vectorizer(inputs[\"answer\"]))\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tmp\n",
    "transformer.save(f'tmp/transformer_{num_layers}_{model_dim}_{dff}_{num_heads}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the transformer if needed\n",
    "#transformer = tf.keras.models.load_model(\"tmp\\\\transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translator = TranslatorWithBeamSearch(vectorizer,vectorizer,transformer,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = tf.constant([\"how are you doing.\"])\n",
    "output=translator(sentence,20)\n",
    "# this will returns the best sentence\n",
    "output[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('machine_learning_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91fb6aa2260365318ef26a47b973b775ccda6a02fb9ff6ae48a05d7381289f0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
