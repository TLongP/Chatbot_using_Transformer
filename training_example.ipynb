{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "from loader import pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from models.transformer.transformer import Transformer\n",
    "from models.custom_metrics.metrics import loss_function, accuracy_function\n",
    "import tensorflow_addons as tfa\n",
    "from translator.translator import TranslatorWithVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset\\set_2\\dialogs.txt\"\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pipeline.create_dataset(data_path,BATCH_SIZE = 128)\n",
    "vectorizer = TextVectorization(max_tokens=5000,standardize=pipeline.add_start_and_end_tokens)\n",
    "vectorizer.adapt(train.map(lambda x: x[\"question\"]))\n",
    "vocab = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "model_dim = 128\n",
    "dff = 256\n",
    "num_heads = 4\n",
    "dropout_rate = 0.1\n",
    "vocab_len = len(vocab)\n",
    "INIT_LR = 1e-4\n",
    "MAX_LR = 1e-2\n",
    "MAX_TOKENS = 128\n",
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    model_dim=model_dim,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_len,\n",
    "    target_vocab_size=vocab_len,\n",
    "    dropout_rate=dropout_rate,\n",
    "    max_tokens=MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, model_dim, warmup_steps=1000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.model_dim = tf.cast(self.model_dim, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.model_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(model_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate, \n",
    "                            beta_1=0.9, \n",
    "                            beta_2=0.98,\n",
    "                            epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_step_signature = [\n",
    "tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer([inp, tar_inp],\n",
    "                    training = True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 7.7400 Accuracy 0.0469\n",
      "Epoch 2 Loss 7.1912 Accuracy 0.1337\n",
      "Epoch 3 Loss 6.6020 Accuracy 0.1333\n",
      "Epoch 4 Loss 5.9891 Accuracy 0.1346\n",
      "Epoch 5 Loss 5.6093 Accuracy 0.1551\n",
      "Epoch 6 Loss 5.4209 Accuracy 0.1665\n",
      "Epoch 7 Loss 5.2014 Accuracy 0.1825\n",
      "Epoch 8 Loss 4.9267 Accuracy 0.2057\n",
      "Epoch 9 Loss 4.6923 Accuracy 0.2214\n",
      "Epoch 10 Loss 4.4753 Accuracy 0.2395\n",
      "Epoch 11 Loss 4.2610 Accuracy 0.2575\n",
      "Epoch 12 Loss 4.0575 Accuracy 0.2751\n",
      "Epoch 13 Loss 3.8435 Accuracy 0.2983\n",
      "Epoch 14 Loss 3.6621 Accuracy 0.3150\n",
      "Epoch 15 Loss 3.4594 Accuracy 0.3376\n",
      "Epoch 16 Loss 3.2567 Accuracy 0.3629\n",
      "Epoch 17 Loss 3.0589 Accuracy 0.3881\n",
      "Epoch 18 Loss 2.8582 Accuracy 0.4105\n",
      "Epoch 19 Loss 2.6631 Accuracy 0.4384\n",
      "Epoch 20 Loss 2.4702 Accuracy 0.4681\n",
      "Epoch 21 Loss 2.2767 Accuracy 0.4966\n",
      "Epoch 22 Loss 2.0859 Accuracy 0.5318\n",
      "Epoch 23 Loss 1.9130 Accuracy 0.5623\n",
      "Epoch 24 Loss 1.7461 Accuracy 0.5904\n",
      "Epoch 25 Loss 1.5880 Accuracy 0.6216\n",
      "Epoch 26 Loss 1.4229 Accuracy 0.6560\n",
      "Epoch 27 Loss 1.3472 Accuracy 0.6652\n",
      "Epoch 28 Loss 1.1967 Accuracy 0.6985\n",
      "Epoch 29 Loss 1.1006 Accuracy 0.7177\n",
      "Epoch 30 Loss 1.0261 Accuracy 0.7354\n",
      "Epoch 31 Loss 0.9595 Accuracy 0.7454\n",
      "Epoch 32 Loss 0.8871 Accuracy 0.7643\n",
      "Epoch 33 Loss 0.8342 Accuracy 0.7727\n",
      "Epoch 34 Loss 0.8241 Accuracy 0.7753\n",
      "Epoch 35 Loss 0.7802 Accuracy 0.7824\n",
      "Epoch 36 Loss 0.7066 Accuracy 0.8023\n",
      "Epoch 37 Loss 0.6323 Accuracy 0.8213\n",
      "Epoch 38 Loss 0.5970 Accuracy 0.8307\n",
      "Epoch 39 Loss 0.5664 Accuracy 0.8404\n",
      "Epoch 40 Loss 0.5355 Accuracy 0.8456\n",
      "Epoch 41 Loss 0.5212 Accuracy 0.8485\n",
      "Epoch 42 Loss 0.4851 Accuracy 0.8589\n",
      "Epoch 43 Loss 0.4613 Accuracy 0.8671\n",
      "Epoch 44 Loss 0.4432 Accuracy 0.8711\n",
      "Epoch 45 Loss 0.4146 Accuracy 0.8797\n",
      "Epoch 46 Loss 0.3916 Accuracy 0.8858\n",
      "Epoch 47 Loss 0.3742 Accuracy 0.8910\n",
      "Epoch 48 Loss 0.3782 Accuracy 0.8896\n",
      "Epoch 49 Loss 0.3544 Accuracy 0.8971\n",
      "Epoch 50 Loss 0.3242 Accuracy 0.9039\n",
      "Epoch 51 Loss 0.3132 Accuracy 0.9076\n",
      "Epoch 52 Loss 0.3084 Accuracy 0.9085\n",
      "Epoch 53 Loss 0.2992 Accuracy 0.9123\n",
      "Epoch 54 Loss 0.2776 Accuracy 0.9185\n",
      "Epoch 55 Loss 0.2856 Accuracy 0.9143\n",
      "Epoch 56 Loss 0.3115 Accuracy 0.9063\n",
      "Epoch 57 Loss 0.2910 Accuracy 0.9117\n",
      "Epoch 58 Loss 0.2742 Accuracy 0.9194\n",
      "Epoch 59 Loss 0.2526 Accuracy 0.9240\n",
      "Epoch 60 Loss 0.2624 Accuracy 0.9234\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for batch, inputs in enumerate(train):\n",
    "        train_step(vectorizer(inputs[\"question\"]), vectorizer(inputs[\"answer\"]))\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002479BBD0220>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002479BBE0D90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, positional_encoding_layer_call_fn, positional_encoding_layer_call_and_return_conditional_losses, dropout_6_layer_call_fn while saving (showing 5 of 152). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp/transformer\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp/transformer\\assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p tmp\n",
    "transformer.save('tmp/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TranslatorWithVectorizer(vectorizer,transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need longer training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] i got a good nose for that\\x97cigarettes stink [END] '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf.constant([\"what did you do?\"])\n",
    "answer,_,_ = translator(test,128)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] i got a good spot [END] '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf.constant([\"where are the band-aids?\"])\n",
    "answer,_,_ = translator(test,128)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('machine_learning_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91fb6aa2260365318ef26a47b973b775ccda6a02fb9ff6ae48a05d7381289f0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
