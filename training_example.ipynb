{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from models.transformer.transformer import Transformer\n",
    "from models.custom_metrics.metrics import loss_function, accuracy_function\n",
    "from translator.translator import TranslatorWithBeamSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset\\set_2\\dialogs.txt\"\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pipeline.create_dataset(data_path,BATCH_SIZE = 128)\n",
    "vectorizer = TextVectorization(max_tokens=5000,standardize=pipeline.add_start_and_end_tokens)\n",
    "vectorizer.adapt(train.map(lambda x: x[\"question\"]))\n",
    "vocab = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "model_dim = 64\n",
    "dff = 128\n",
    "num_heads = 4\n",
    "dropout_rate = 0.2\n",
    "vocab_len = len(vocab)\n",
    "INIT_LR = 1e-4\n",
    "MAX_LR = 1e-2\n",
    "MAX_TOKENS = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    model_dim=model_dim,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_len,\n",
    "    target_vocab_size=vocab_len,\n",
    "    dropout_rate=dropout_rate,\n",
    "    max_tokens=MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, model_dim, warmup_steps=1000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.model_dim = tf.cast(self.model_dim, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.model_dim) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(model_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "                            learning_rate, \n",
    "                            beta_1=0.9, \n",
    "                            beta_2=0.98,\n",
    "                            epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer([inp, tar_inp],\n",
    "                    training = True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 7.6936 Accuracy 0.0217\n",
      "Epoch 2 Loss 6.9952 Accuracy 0.5117\n",
      "Epoch 3 Loss 6.2475 Accuracy 0.5235\n",
      "Epoch 4 Loss 5.2906 Accuracy 0.5337\n",
      "Epoch 5 Loss 4.2283 Accuracy 0.5383\n",
      "Epoch 6 Loss 3.5042 Accuracy 0.5289\n",
      "Epoch 7 Loss 3.3994 Accuracy 0.4726\n",
      "Epoch 8 Loss 5.6091 Accuracy 0.1331\n",
      "Epoch 9 Loss 5.2025 Accuracy 0.1913\n",
      "Epoch 10 Loss 4.9232 Accuracy 0.2182\n",
      "Epoch 11 Loss 4.6334 Accuracy 0.2551\n",
      "Epoch 12 Loss 4.4981 Accuracy 0.2653\n",
      "Epoch 13 Loss 4.2874 Accuracy 0.2866\n",
      "Epoch 14 Loss 4.2470 Accuracy 0.2817\n",
      "Epoch 15 Loss 4.0993 Accuracy 0.2919\n",
      "Epoch 16 Loss 3.9797 Accuracy 0.3000\n",
      "Epoch 17 Loss 3.8588 Accuracy 0.3087\n",
      "Epoch 18 Loss 3.7097 Accuracy 0.3222\n",
      "Epoch 19 Loss 3.6103 Accuracy 0.3293\n",
      "Epoch 20 Loss 3.4736 Accuracy 0.3472\n",
      "Epoch 21 Loss 3.3481 Accuracy 0.3580\n",
      "Epoch 22 Loss 3.2084 Accuracy 0.3699\n",
      "Epoch 23 Loss 3.1188 Accuracy 0.3786\n",
      "Epoch 24 Loss 2.9708 Accuracy 0.3971\n",
      "Epoch 25 Loss 2.8620 Accuracy 0.4065\n",
      "Epoch 26 Loss 2.7253 Accuracy 0.4250\n",
      "Epoch 27 Loss 2.6455 Accuracy 0.4322\n",
      "Epoch 28 Loss 2.5624 Accuracy 0.4414\n",
      "Epoch 29 Loss 2.4444 Accuracy 0.4586\n",
      "Epoch 30 Loss 2.3158 Accuracy 0.4782\n",
      "Epoch 31 Loss 2.2499 Accuracy 0.4901\n",
      "Epoch 32 Loss 2.1691 Accuracy 0.5003\n",
      "Epoch 33 Loss 2.0998 Accuracy 0.5107\n",
      "Epoch 34 Loss 2.0208 Accuracy 0.5218\n",
      "Epoch 35 Loss 1.9759 Accuracy 0.5292\n",
      "Epoch 36 Loss 1.9099 Accuracy 0.5378\n",
      "Epoch 37 Loss 1.8570 Accuracy 0.5519\n",
      "Epoch 38 Loss 1.7782 Accuracy 0.5638\n",
      "Epoch 39 Loss 1.6924 Accuracy 0.5816\n",
      "Epoch 40 Loss 1.6714 Accuracy 0.5864\n",
      "Epoch 41 Loss 1.6176 Accuracy 0.5980\n",
      "Epoch 42 Loss 1.5855 Accuracy 0.6005\n",
      "Epoch 43 Loss 1.5295 Accuracy 0.6112\n",
      "Epoch 44 Loss 1.5251 Accuracy 0.6123\n",
      "Epoch 45 Loss 1.4722 Accuracy 0.6201\n",
      "Epoch 46 Loss 1.4253 Accuracy 0.6311\n",
      "Epoch 47 Loss 1.4161 Accuracy 0.6338\n",
      "Epoch 48 Loss 1.3941 Accuracy 0.6355\n",
      "Epoch 49 Loss 1.3708 Accuracy 0.6406\n",
      "Epoch 50 Loss 1.3223 Accuracy 0.6503\n",
      "Epoch 51 Loss 1.3031 Accuracy 0.6535\n",
      "Epoch 52 Loss 1.2967 Accuracy 0.6534\n",
      "Epoch 53 Loss 1.2623 Accuracy 0.6628\n",
      "Epoch 54 Loss 1.2353 Accuracy 0.6705\n",
      "Epoch 55 Loss 1.2479 Accuracy 0.6635\n",
      "Epoch 56 Loss 1.2210 Accuracy 0.6708\n",
      "Epoch 57 Loss 1.1912 Accuracy 0.6785\n",
      "Epoch 58 Loss 1.1747 Accuracy 0.6809\n",
      "Epoch 59 Loss 1.1391 Accuracy 0.6929\n",
      "Epoch 60 Loss 1.1416 Accuracy 0.6882\n",
      "Epoch 61 Loss 1.1208 Accuracy 0.6957\n",
      "Epoch 62 Loss 1.1079 Accuracy 0.6957\n",
      "Epoch 63 Loss 1.1048 Accuracy 0.6994\n",
      "Epoch 64 Loss 1.0777 Accuracy 0.7024\n",
      "Epoch 65 Loss 1.0832 Accuracy 0.7012\n",
      "Epoch 66 Loss 1.0637 Accuracy 0.7039\n",
      "Epoch 67 Loss 1.0587 Accuracy 0.7074\n",
      "Epoch 68 Loss 1.0266 Accuracy 0.7130\n",
      "Epoch 69 Loss 1.0128 Accuracy 0.7142\n",
      "Epoch 70 Loss 1.0177 Accuracy 0.7134\n",
      "Epoch 71 Loss 0.9932 Accuracy 0.7248\n",
      "Epoch 72 Loss 0.9834 Accuracy 0.7225\n",
      "Epoch 73 Loss 0.9926 Accuracy 0.7212\n",
      "Epoch 74 Loss 0.9487 Accuracy 0.7339\n",
      "Epoch 75 Loss 0.9620 Accuracy 0.7264\n",
      "Epoch 76 Loss 0.9540 Accuracy 0.7281\n",
      "Epoch 77 Loss 0.9421 Accuracy 0.7334\n",
      "Epoch 78 Loss 0.9182 Accuracy 0.7382\n",
      "Epoch 79 Loss 0.9102 Accuracy 0.7419\n",
      "Epoch 80 Loss 0.9008 Accuracy 0.7437\n",
      "Epoch 81 Loss 0.9023 Accuracy 0.7471\n",
      "Epoch 82 Loss 0.8883 Accuracy 0.7464\n",
      "Epoch 83 Loss 0.8572 Accuracy 0.7556\n",
      "Epoch 84 Loss 0.8619 Accuracy 0.7513\n",
      "Epoch 85 Loss 0.8670 Accuracy 0.7520\n",
      "Epoch 86 Loss 0.8828 Accuracy 0.7483\n",
      "Epoch 87 Loss 0.8595 Accuracy 0.7530\n",
      "Epoch 88 Loss 0.8579 Accuracy 0.7555\n",
      "Epoch 89 Loss 0.8423 Accuracy 0.7619\n",
      "Epoch 90 Loss 0.8284 Accuracy 0.7618\n",
      "Epoch 91 Loss 0.8154 Accuracy 0.7619\n",
      "Epoch 92 Loss 0.8234 Accuracy 0.7649\n",
      "Epoch 93 Loss 0.8317 Accuracy 0.7576\n",
      "Epoch 94 Loss 0.8031 Accuracy 0.7691\n",
      "Epoch 95 Loss 0.8158 Accuracy 0.7627\n",
      "Epoch 96 Loss 0.8024 Accuracy 0.7670\n",
      "Epoch 97 Loss 0.7915 Accuracy 0.7718\n",
      "Epoch 98 Loss 0.7947 Accuracy 0.7680\n",
      "Epoch 99 Loss 0.7857 Accuracy 0.7716\n",
      "Epoch 100 Loss 0.7896 Accuracy 0.7724\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for batch, inputs in enumerate(train):\n",
    "        train_step(vectorizer(inputs[\"question\"]), vectorizer(inputs[\"answer\"]))\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000222EE0C40A0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x00000222EE0D5BE0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, positional_encoding_layer_call_fn, positional_encoding_layer_call_and_return_conditional_losses, dropout_6_layer_call_fn while saving (showing 5 of 152). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp/transformer_2_64_128_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp/transformer_2_64_128_4\\assets\n"
     ]
    }
   ],
   "source": [
    "!mkdir tmp\n",
    "transformer.save(f'tmp/transformer_{num_layers}_{model_dim}_{dff}_{num_heads}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from models.transformer.transformer import Transformer\n",
    "from models.custom_metrics.metrics import loss_function, accuracy_function, loss_function_no_mask\n",
    "from translator.translator import TranslatorWithBeamSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset\\set_2\\dialogs.txt\"\n",
    "BATCH_SIZE = 128\n",
    "train = pipeline.create_dataset(data_path,BATCH_SIZE = 128)\n",
    "vectorizer = TextVectorization(max_tokens=5000,standardize=pipeline.add_start_and_end_tokens)\n",
    "vectorizer.adapt(train.map(lambda x: x[\"question\"]))\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "# load the transformer if needed\n",
    "#transformer = tf.keras.models.load_model(\"tmp\\\\transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translator = TranslatorWithBeamSearch(vectorizer,vectorizer,transformer,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thank you very much for you very much i'll pass as news with me about [UNK] i'm very much very much see you very much see you very much\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = tf.constant([\"good luck with that.\"])\n",
    "output=translator(sentence,30)\n",
    "# this will returns the best sentence\n",
    "output[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('machine_learning_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91fb6aa2260365318ef26a47b973b775ccda6a02fb9ff6ae48a05d7381289f0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
